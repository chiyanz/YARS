{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f65f7b-a854-410e-a5ff-a4e1c3a05420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import requests\n",
    "import os\n",
    "from os import listdir\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a43fa87-eccd-4664-9cd0-85a290cf7f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6382db5-6e90-4710-87c8-56d9eea058d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"yars_data/\"\n",
    "def gather_photos(label: str, no_captions: bool = False):\n",
    "    \"\"\"\n",
    "    takes the label (category of images to fetch) and filters photos.json for images with that label\n",
    "    filters for images that \n",
    "    returns: list of photo objects (photo_id, label, caption, etc.)\n",
    "    \"\"\" \n",
    "    with open(os.path.join(root_dir, 'photos.json'), 'r') as file:\n",
    "        all_photos = []\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            try:\n",
    "                photo_record = json.loads(line)\n",
    "                # filers for only ones wiht captions by default\n",
    "                has_captions = len(photo_record['caption']) > 0 or no_captions \n",
    "                if photo_record['label'] == label and has_captions:\n",
    "                    all_photos.append(photo_record[\"photo_id\"] + \".jpg\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e} at line: {line}\")\n",
    "        return all_photos\n",
    "labeled_test = gather_photos('food', no_captions = False)[:100]\n",
    "unlabeled_test = gather_photos('food', no_captions = True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e993e05-edd0-4cf1-871a-7bd4938cc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_chunks(l, n): \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732362fd-9e3e-46d2-94a4-a749789f4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 100 of 100"
     ]
    }
   ],
   "source": [
    "# if running for the first time\n",
    "\n",
    "# generating test set feature extraction for 100 labeled images\n",
    "image_dir = root_dir + \"photos/\"\n",
    "chunk_size = 20\n",
    "chunks = divide_chunks(labeled_test, chunk_size)\n",
    "\n",
    "paths  = []\n",
    "failed = []\n",
    "features = torch.zeros(1, 768)\n",
    "for chunk in chunks:\n",
    "    images = []\n",
    "    for path in chunk:\n",
    "        try:\n",
    "            images.append(Image.open(image_dir + path))\n",
    "            paths.append(path)\n",
    "            print(\"\\rLoading image %d of %d\" % (len(paths) + len(failed), len(labeled_test)), end=\"\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\r                                  - Failed to load image %s\" % image_dir + path, end=\"\")\n",
    "            failed.append(path)\n",
    "    print(\"\\rProcessing image %d of %d\" % (len(paths) + len(failed), len(labeled_test)), end=\"\")\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    features = torch.cat((features, image_features), dim=0)\n",
    "\n",
    "    # write data to file\n",
    "    path_out    = \"test/caption_paths.txt\"\n",
    "    feature_out = \"test/caption_features.pt\" # this is a binary file, shape is (n_images, 768)\n",
    "    \n",
    "    with open(path_out, \"w\") as f:\n",
    "        for path in paths:\n",
    "            f.write(path + \"\\n\")\n",
    "    \n",
    "    with open(feature_out, \"wb\") as f:\n",
    "        torch.save(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b5a4d7-5a1e-4250-b404-969a66215e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 100 of 100"
     ]
    }
   ],
   "source": [
    "# generating test set feature extraction for 100 unlabeled images\n",
    "image_dir = root_dir + \"photos/\"\n",
    "chunk_size = 20\n",
    "chunks = divide_chunks(unlabeled_test, chunk_size)\n",
    "\n",
    "paths  = []\n",
    "failed = []\n",
    "features = torch.zeros(1, 768)\n",
    "for chunk in chunks:\n",
    "    images = []\n",
    "    for path in chunk:\n",
    "        try:\n",
    "            images.append(Image.open(image_dir + path))\n",
    "            paths.append(path)\n",
    "            print(\"\\rLoading image %d of %d\" % (len(paths) + len(failed), len(unlabeled_test)), end=\"\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\r                                  - Failed to load image %s\" % path, end=\"\")\n",
    "            failed.append(path)\n",
    "    print(\"\\rProcessing image %d of %d\" % (len(paths) + len(failed), len(unlabeled_test)), end=\"\")\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    features = torch.cat((features, image_features), dim=0)\n",
    "\n",
    "    # write data to file\n",
    "    path_out    = \"test/nocaption_paths.txt\"\n",
    "    feature_out = \"test/nocaption_features.pt\" # this is a binary file, shape is (n_images, 768)\n",
    "    \n",
    "    with open(path_out, \"w\") as f:\n",
    "        for path in paths:\n",
    "            f.write(path + \"\\n\")\n",
    "    \n",
    "    with open(feature_out, \"wb\") as f:\n",
    "        torch.save(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cefb9859-4c3f-4f43-b7b2-3a510543829f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1842262340.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    write data to file\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# if continuing job\n",
    "path_out    = \"out/paths.txt\"\n",
    "feature_out = \"out/features.pt\" # this is a binary file, shape is (n_images, 768)\n",
    "paths  = open(path_out).read().split(\"\\n\")\n",
    "num_processed = len(paths) - 1\n",
    "all_photos = all_photos[num_processed:]\n",
    "image_dir = root_dir + \"photos/\"\n",
    "chunk_size = 20\n",
    "chunks = divide_chunks(all_photos, chunk_size)\n",
    "\n",
    "\n",
    "failed = []\n",
    "features = torch.load(feature_out)\n",
    "for chunk in chunks:\n",
    "    images = []\n",
    "    for path in chunk:\n",
    "        try:\n",
    "            images.append(Image.open(image_dir + path))\n",
    "            paths.append(path)\n",
    "            print(\"\\rLoading image %d of %d\" % (len(paths) + len(failed), len(all_photos)), end=\"\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"\\r                                  - Failed to load image %s\" % path, end=\"\")\n",
    "            failed.append(path)\n",
    "    print(\"\\rProcessing image %d of %d\" % (len(paths) + len(failed), len(all_photos)), end=\"\")\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    features = torch.cat((features, image_features), dim=0)\n",
    "\n",
    "    write data to file\n",
    "    with open(path_out, \"w\") as f:\n",
    "        for path in paths:\n",
    "            f.write(path + \"\\n\")\n",
    "    \n",
    "    with open(feature_out, \"wb\") as f:\n",
    "        torch.save(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84752a3a-9b47-461d-bef5-6f99260aa621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498dcb03-0302-4d3d-8410-61df501a39c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
