{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2000e77-286b-43e6-a2e6-c25821c5062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CoCa(\n",
       "  (text): TextTransformer(\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attn_pool): AttentionalPooler(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ln_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_k): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): MultimodalTransformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (cross_attn): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_1_kv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "model, _, transform = open_clip.create_model_and_transforms(\n",
    "  model_name=\"coca_ViT-L-14\",\n",
    "  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c9a9c6-3add-4eae-99ef-69feea80b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yars_data/photos/pve7D6NUrafHW3EAORubyw.jpg',\n",
       " 'yars_data/photos/Le9rMdT8YFlvqr431LctIQ.jpg',\n",
       " 'yars_data/photos/9kVdBkGWcKfCFzSwUXjQyw.jpg',\n",
       " 'yars_data/photos/e0dD0np3hY3F8LoUtrNoPw.jpg',\n",
       " 'yars_data/photos/xiyqMEgTl4B4ux047E_zqw.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "root_image_path = \"yars_data/photos/\"\n",
    "root_path = \"yars_data\"\n",
    "with open('out/paths.txt', 'r') as file:\n",
    "    file_names = file.readlines()\n",
    "    \n",
    "image_paths = [os.path.join(root_image_path + file_name.strip()) for file_name in file_names[1:]]\n",
    "\n",
    "# Display the combined paths (normally you would load images using a library like PIL or OpenCV)\n",
    "image_paths[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1b0976-8a06-4c91-966f-22c65926bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yars_data/photos/H52Er-uBg6rNrHcReWTD2w.jpg']\n"
     ]
    }
   ],
   "source": [
    "def gather_photos(label: str, no_captions: bool):\n",
    "    \"\"\"\n",
    "    takes the label (category of images to fetch) and filters photos.json for images with that label\n",
    "    filters for images that \n",
    "    returns: list of photo objects (photo_id, label, caption, etc.)\n",
    "    \"\"\" \n",
    "    with open(os.path.join(root_path, 'photos.json'), 'r') as file:\n",
    "        all_photos = []\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            try:\n",
    "                photo_record = json.loads(line)\n",
    "                # filers for only ones wiht captions by default\n",
    "                has_captions = (len(photo_record['caption']) > 0) != no_captions\n",
    "                if photo_record['label'] == label and has_captions:\n",
    "                    all_photos.append(root_image_path + photo_record[\"photo_id\"] + \".jpg\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e} at line: {line}\")\n",
    "        return all_photos\n",
    "no_caption_photos = gather_photos('food', no_captions=True)\n",
    "caption_photos = gather_photos('food', no_captions=False)\n",
    "print(no_caption_photos[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470a17c5-0b8c-47a6-a61e-b7ef9986f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yars_data/photos/4Zia9NkAfQNjMfcIDhwJ-g.jpg\n",
      "Filtered Image IDs: []\n"
     ]
    }
   ],
   "source": [
    "# continue generating captions\n",
    "def read_last_line(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        f.seek(-2, 2)  # Jump to the second last byte.\n",
    "        while f.read(1) != b'\\n':  # Until EOL is found...\n",
    "            f.seek(-2, 1)\n",
    "        last_line = f.readline().decode()\n",
    "    return last_line\n",
    "\n",
    "last_line = read_last_line('captions.txt')\n",
    "\n",
    "def extract_image_path(last_line):\n",
    "    # Extract everything before the colon which includes the path and image_id\n",
    "    image_path = last_line.split(':')[0].strip()\n",
    "    return image_path\n",
    "\n",
    "def slice_image_ids(image_ids, start_id):\n",
    "    if start_id in image_ids:\n",
    "        start_index = image_ids.index(start_id)\n",
    "        return image_ids[start_index + 1:]\n",
    "    else:\n",
    "        return []  # or return image_ids to start from the beginning if ID not found\n",
    "\n",
    "\n",
    "image_id = extract_image_path(last_line)\n",
    "print(image_id)\n",
    "no_caption_photos = slice_image_ids(no_caption_photos, image_id)[1:]\n",
    "print(\"Filtered Image IDs:\", no_caption_photos[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5857c9c4-6ea1-4beb-b598-dc8584d9c379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "captions = []\n",
    "total_photos = len(no_caption_photos)\n",
    "\n",
    "print(total_photos)\n",
    "\n",
    "with open(\"captions.txt\", \"a\") as file:\n",
    "    for i, image_path in enumerate(no_caption_photos):\n",
    "        im = Image.open(image_path).convert(\"RGB\")\n",
    "        im = transform(im).unsqueeze(0)\n",
    "        im = im.to(device)\n",
    "\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "          generated = model.generate(im, generation_type='top_k')\n",
    "        \n",
    "        caption = open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\")\n",
    "        captions.append({\"image\": image_path, \"caption\": caption})\n",
    "        file.write(f\"{image_path}: {caption}\\n\")\n",
    "        file.flush()\n",
    "        \n",
    "        print(f\"Processed {i+1}/{total_photos}: {image_path}\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40a775e-03ef-434a-944c-6af7343d1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"captions.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        image_data = line.split(\":\")\n",
    "        img_id = image_data[0].split(\"/\")[-1]\n",
    "        \n",
    "# ignore for now\n",
    "def label_photos(photos, captions):\n",
    "    \"\"\"\n",
    "    takes the list of (previously uncaptioned) pictures, apply the captions to them, and create a new image set\n",
    "    returns: None\n",
    "    \"\"\" \n",
    "    with open(os.path.join(root_path, 'photos.json'), 'r') as file:\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            try:\n",
    "                photo_record = json.loads(line)\n",
    "                # filers for only ones wiht captions by default\n",
    "                has_captions = len(photo_record['caption']) > 0 != no_captions\n",
    "                if photo_record['label'] == label and has_captions:\n",
    "                    all_photos.append(root_image_path + photo_record[\"photo_id\"] + \".jpg\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e} at line: {line}\")\n",
    "        return all_photos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
